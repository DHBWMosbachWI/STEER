{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from data_loader.hybrid_data_loaders import *\n",
    "from data_loader.header_data_loaders import *\n",
    "#from data_loader.CT_Wiki_data_loaders import *\n",
    "from data_loader.RE_data_loaders import *\n",
    "from data_loader.EL_data_loaders import *\n",
    "from model.configuration import TableConfig\n",
    "from model.model import HybridTableMaskedLM, HybridTableCER, TableHeaderRanking, HybridTableCT,HybridTableEL,HybridTableRE,BertRE\n",
    "from model.transformers import BertConfig,BertTokenizer, WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from utils.util import *\n",
    "from baselines.row_population.metric import average_precision,ndcg_at_k\n",
    "from baselines.cell_filling.cell_filling import *\n",
    "from model import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'CER': (TableConfig, HybridTableCER, BertTokenizer),\n",
    "    'CF' : (TableConfig, HybridTableMaskedLM, BertTokenizer),\n",
    "    'HR': (TableConfig, TableHeaderRanking, BertTokenizer),\n",
    "    'CT': (TableConfig, HybridTableCT, BertTokenizer),\n",
    "    'EL': (TableConfig, HybridTableEL, BertTokenizer),\n",
    "    'RE': (TableConfig, HybridTableRE, BertTokenizer),\n",
    "    'REBERT': (BertConfig, BertRE, BertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory, this will be used to load test data\n",
    "data_dir = 'data/wikitables_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"configs/table-base-config_v2.json\"\n",
    "device = torch.device('cuda')\n",
    "# load entity vocab from entity_vocab.txt\n",
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "entity_wikid2id = {entity_vocab[x]['wiki_id']:x for x in entity_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "This notebook shows examples of how to using the model components and running evaluation of different tasks.\n",
    "* [Pretrained and Cell Filling](#cf)\n",
    "* [Entity Linking](#el)\n",
    "* [Column Type Classification](#ct)\n",
    "* [Relation Extraction](#re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ct\"></a>\n",
    "# CT\n",
    "Evaluate column type annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.CT_Wiki_data_loaders_STEER import WikiCTDataset, CTLoader\n",
    "type_vocab = load_type_vocab(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WikiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=1, unlabeled_data_size=\"absolute\", test_data_size=20.0, random_state=1, data_split_set=\"labeled\", add_STEER_train_data=True, max_input_tok=500, src=\"train_dev_test\", max_length = [50, 10, 10], force_new=False, tokenizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.df[\"table_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "train_sampler = SequentialSampler(data) if -1 == -1 else DistributedSampler(train_dataset)\n",
    "train_dataloader = CTLoader(data, sampler=train_sampler, batch_size=1, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "train_sampler = SequentialSampler(data) if -1 == -1 else DistributedSampler(train_dataset)\n",
    "train_dataloader = CTLoader(data, sampler=train_sampler, batch_size=1, is_train=True)\n",
    "\n",
    "epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "mode = 0\n",
    "print(mode)\n",
    "config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.class_num = len(type_vocab)\n",
    "config.mode = mode\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = checkpoints[mode]\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    if step > 0:\n",
    "        break\n",
    "    table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "            input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "            column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'test.table_col_type.json'), 'r') as f:\n",
    "    testset = json.load(f)[4]\n",
    "table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations = testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_columns = [0,2]\n",
    "list(map(type_annotations.__getitem__, labeled_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import os\n",
    "os.environ[\"WORKING_DIR\"] = \"/home/sanonymous/semantic_data_lake\"\n",
    "os.environ[\"TYPENAME\"] = \"type_turl\"\n",
    "corpus = \"turl\"\n",
    "labeled_data_size = 1\n",
    "unlabeled_data_size = \"absolute\"\n",
    "test_data_size = 20.0\n",
    "random_state = 1\n",
    "gen_train_data = False\n",
    "absolute_numbers = True\n",
    "validation_on = \"test\"\n",
    "\n",
    "valid_header_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\",\n",
    "                         \"valid_headers\")\n",
    "\n",
    "# load the valid headers with real sem. types\n",
    "valid_header_file = f\"{corpus}_{os.environ['TYPENAME']}_valid.json\"\n",
    "valid_headers = join(valid_header_path, valid_header_file)\n",
    "with open(valid_headers, \"r\") as file:\n",
    "    valid_headers = json.load(file)\n",
    "\n",
    "new_valid_headers = {}\n",
    "\n",
    "for table in valid_headers.keys():\n",
    "    new_valid_headers[table.split(\"_\")[1].split(\".csv\")[0]] = valid_headers[table]\n",
    "valid_headers = new_valid_headers\n",
    "\n",
    "labeled_unlabeled_test_split_path = join(os.environ[\"WORKING_DIR\"], \"data\",\n",
    "                                         \"extract\", \"out\",\n",
    "                                         \"labeled_unlabeled_test_split\")\n",
    "\n",
    "# load labeled data from labeled, unlabeled, test split file and use labeled and test data for clustering\n",
    "with open(\n",
    "        join(\n",
    "            labeled_unlabeled_test_split_path,\n",
    "            f\"{corpus}_{labeled_data_size}_{unlabeled_data_size}_{test_data_size}_{random_state}.json\"\n",
    "        )) as f:\n",
    "    labeled_unlabeled_test_split_file = json.load(f)\n",
    "    labeled_data_ids = labeled_unlabeled_test_split_file[\n",
    "        f\"labeled{labeled_data_size}\"]\n",
    "    if gen_train_data:\n",
    "        if absolute_numbers:\n",
    "            unlabeled_data_ids = labeled_unlabeled_test_split_file[\n",
    "                f\"unlabeled\"]\n",
    "        else:\n",
    "            unlabeled_data_ids = labeled_unlabeled_test_split_file[\n",
    "                f\"unlabeled{unlabeled_data_size}\"]\n",
    "        print(f\"Unlabeled Data: {len(unlabeled_data_ids)}\")\n",
    "    if validation_on == \"unlabeled\":\n",
    "        test_data_ids = labeled_unlabeled_test_split_file[\n",
    "            f\"{validation_on}\"]\n",
    "    else:\n",
    "        test_data_ids = labeled_unlabeled_test_split_file[\n",
    "            f\"{validation_on}{test_data_size}\"]\n",
    "\n",
    "print(f\"Labeled Data: {len(labeled_data_ids)}\")\n",
    "print(f\"Test Data: {len(test_data_ids)}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"table_id\": [entry.split(\"+\")[0].split(\".csv\")[0].split(\"_\")[1] for entry in labeled_unlabeled_test_split_file[\"labeled1\"]],\n",
    "                  \"column\": [entry.split(\"+\")[1].split(\"_\")[1] for entry in labeled_unlabeled_test_split_file[\"labeled1\"]]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load additional STEER train data\n",
    "gen_train_path = join(os.environ[\"WORKING_DIR\"], \"labeling_functions\", \"combined_LFs\", \"gen_training_data\", f\"{corpus}_gen_training_data_all_combined_maj_{labeled_data_size}_absolute_{test_data_size}_{random_state}.csv\")\n",
    "\n",
    "df_gen_train_data = pd.read_csv(gen_train_path, names=[\"table_id\", \"column\", \"dataset_id\", \"predicted_semantic_type\"], header=0)\n",
    "df_gen_train_data[\"table_id\"] = df_gen_train_data[\"table_id\"].apply(lambda x: x.split(\".csv\")[0].split(\"_\")[1])\n",
    "df_gen_train_data[\"column\"] = df_gen_train_data[\"column\"].apply(lambda x: int(x.split(\"_\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen_train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, df_gen_train_data[[\"table_id\", \"column\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\", \"valid_types\", \"types.json\")) as f:\n",
    "    types = json.load(f)[\"type_turl\"]\n",
    "df_types = pd.DataFrame({\"types\":types})\n",
    "df_types.to_csv(join(data_dir, \"types_STEER.txt\"), header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_to_one_valid_header(input_data, valid_headers):\n",
    "    table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations = input_data\n",
    "    \n",
    "    valid_header_entry = valid_headers.get(table_id)\n",
    "    if valid_header_entry == None:\n",
    "        return None\n",
    "    column_ids = [int(col.split(\"_\")[1]) for col in valid_header_entry.keys()]\n",
    "    for column_id in column_ids:\n",
    "        type_annotations[column_id] = [valid_header_entry[f\"column_{column_id}\"][\"semanticType\"]]\n",
    "    \n",
    "    return [table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations]\n",
    "\n",
    "\n",
    "def filter_labeled_unlabeled_test_data(input_data, labeled_unlabeled_test):\n",
    "    df = pd.DataFrame({\"table_id\": [entry.split(\"+\")[0][:-4] for entry in labeled_unlabeled_test[\"labeled1\"]],\n",
    "                  \"column\": [int(entry.split(\"+\")[1].split(\"_\")[1]) for entry in labeled_unlabeled_test[\"labeled1\"]]})\n",
    "\n",
    "    table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations = input_data\n",
    "    if table_id not in df[\"table_id\"].tolist():\n",
    "        return None\n",
    "    labeled_columns = df[\"column\"].tolist()\n",
    "    labeled_columns.sort()\n",
    "    headers = [headers[labeled_column] for labeled_column in labeled_columns]\n",
    "    entities = [entities[labeled_column] for labeled_column in labeled_columns]\n",
    "    # reset column indexes \n",
    "    for i,col in enumerate(entities):\n",
    "        #print([[[row_i,i], cell] for [row_i, col_i], cell in col])\n",
    "        entities[i] = [[[row_i,i], cell] for [row_i, col_i], cell in col]\n",
    "\n",
    "    type_annotations = [type_annotations[labeled_column] for labeled_column in labeled_columns]\n",
    "    return [table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sven: Test WikiCTDataset\n",
    "def process_single_CT(input_data, config):\n",
    "    input_data = overwrite_to_one_valid_header(input_data, valid_headers)\n",
    "    if input_data == None:\n",
    "        return None\n",
    "    input_data = filter_labeled_unlabeled_test_data(input_data, labeled_unlabeled_test_split_file)\n",
    "    if input_data == None:\n",
    "        return None\n",
    "    table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations = input_data\n",
    "    entities = [z for column in entities for z in column[:config.max_column]]\n",
    "    pgEnt = config.entity_wikid2id.get(pgEnt, -1)\n",
    "\n",
    "    tokenized_pgTitle = config.tokenizer.encode(pgTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_meta = tokenized_pgTitle+\\\n",
    "                    config.tokenizer.encode(secTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    if caption != secTitle:\n",
    "        tokenized_meta += config.tokenizer.encode(caption, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_headers = [config.tokenizer.encode(z, max_length=config.max_header_length, add_special_tokens=False) for z in headers]\n",
    "    input_tok = []\n",
    "    input_tok_pos = []\n",
    "    input_tok_type = []\n",
    "    tokenized_meta_length = len(tokenized_meta)\n",
    "    input_tok += tokenized_meta\n",
    "    input_tok_pos += list(range(tokenized_meta_length))\n",
    "    input_tok_type += [0]*tokenized_meta_length\n",
    "    tokenized_headers_length = [len(z) for z in tokenized_headers]\n",
    "    input_tok += list(itertools.chain(*tokenized_headers))\n",
    "    input_tok_pos += list(itertools.chain(*[list(range(z)) for z in tokenized_headers_length]))\n",
    "    input_tok_type += [1]*sum(tokenized_headers_length)\n",
    "\n",
    "    input_ent = []\n",
    "    input_ent_text = []\n",
    "    input_ent_type = []\n",
    "    column_en_map = {}\n",
    "    row_en_map = {}\n",
    "    for e_i, (index, cell) in enumerate(entities):\n",
    "        entity, entity_text = cell\n",
    "        entity = config.entity_wikid2id.get(entity, 0)\n",
    "        tokenized_ent_text = config.tokenizer.encode(entity_text, max_length=config.max_cell_length, add_special_tokens=False)\n",
    "        input_ent.append(entity)\n",
    "        input_ent_text.append(tokenized_ent_text)\n",
    "        input_ent_type.append(4)\n",
    "        if index[1] not in column_en_map:\n",
    "            column_en_map[index[1]] = [e_i]\n",
    "        else:\n",
    "            column_en_map[index[1]].append(e_i)\n",
    "        if index[0] not in row_en_map:\n",
    "            row_en_map[index[0]] = [e_i]\n",
    "        else:\n",
    "            row_en_map[index[0]].append(e_i)\n",
    "    input_ent_length = len(input_ent)\n",
    "    # create column entity mask\n",
    "    column_entity_mask = np.zeros([len(type_annotations), len(input_ent)], dtype=int)\n",
    "    for j in range(len(type_annotations)):\n",
    "        for e_i_1 in column_en_map[j]:\n",
    "            column_entity_mask[j, e_i_1] = 1\n",
    "    # create column header mask\n",
    "    start_i = 0\n",
    "    header_span = {}\n",
    "    column_header_mask = np.zeros([len(type_annotations), len(input_tok)], dtype=int)\n",
    "    for j in range(len(type_annotations)):\n",
    "        header_span[j] = (start_i, start_i+tokenized_headers_length[j])\n",
    "        column_header_mask[j, tokenized_meta_length+header_span[j][0]:tokenized_meta_length+header_span[j][1]] = 1\n",
    "        start_i += tokenized_headers_length[j]\n",
    "    #create input mask\n",
    "    tok_tok_mask = np.ones([len(input_tok), len(input_tok)], dtype=int)\n",
    "    meta_ent_mask = np.ones([tokenized_meta_length, len(input_ent)], dtype=int)\n",
    "    header_ent_mask = np.zeros([sum(tokenized_headers_length), len(input_ent)], dtype=int)\n",
    "    \n",
    "    for e_i, (index, _) in enumerate(entities):\n",
    "        header_ent_mask[header_span[index[1]][0]:header_span[index[1]][1], e_i] = 1\n",
    "    ent_header_mask = np.transpose(header_ent_mask)\n",
    "\n",
    "    input_tok_mask = [tok_tok_mask, np.concatenate([meta_ent_mask, header_ent_mask], axis=0)]\n",
    "    ent_meta_mask = np.ones([len(input_ent), tokenized_meta_length], dtype=int)\n",
    "    \n",
    "    ent_ent_mask = np.eye(len(input_ent), dtype=int)\n",
    "    for _,e_is in column_en_map.items():\n",
    "        for e_i_1 in e_is:\n",
    "            for e_i_2 in e_is:\n",
    "                ent_ent_mask[e_i_1, e_i_2] = 1\n",
    "    for _,e_is in row_en_map.items():\n",
    "        for e_i_1 in e_is:\n",
    "            for e_i_2 in e_is:\n",
    "                ent_ent_mask[e_i_1, e_i_2] = 1\n",
    "    input_ent_mask = [np.concatenate([ent_meta_mask, ent_header_mask], axis=1), ent_ent_mask]\n",
    "    # prepend pgEnt to input_ent, input_ent[0] = pgEnt\n",
    "    if pgEnt!=-1:\n",
    "        input_tok_mask[1] = np.concatenate([np.ones([len(input_tok), 1], dtype=int),input_tok_mask[1]],axis=1)\n",
    "    else:\n",
    "        input_tok_mask[1] = np.concatenate([np.zeros([len(input_tok), 1], dtype=int),input_tok_mask[1]],axis=1)\n",
    "    input_ent = [pgEnt if pgEnt!=-1 else 0] + input_ent\n",
    "    input_ent_text = [tokenized_pgTitle[:config.max_cell_length]] + input_ent_text\n",
    "    input_ent_type = [2] + input_ent_type\n",
    "\n",
    "    new_input_ent_mask = [np.ones([len(input_ent), len(input_tok)], dtype=int), np.ones([len(input_ent), len(input_ent)], dtype=int)]\n",
    "    new_input_ent_mask[0][1:, :] = input_ent_mask[0]\n",
    "    new_input_ent_mask[1][1:, 1:] = input_ent_mask[1]\n",
    "    if pgEnt==-1:\n",
    "        new_input_ent_mask[1][:, 0] = 0\n",
    "        new_input_ent_mask[1][0, :] = 0\n",
    "    column_entity_mask = np.concatenate([np.zeros([len(type_annotations), 1], dtype=int),column_entity_mask],axis=1)\n",
    "\n",
    "    input_ent_mask = new_input_ent_mask\n",
    "    labels = np.zeros([len(type_annotations), config.type_num], dtype=int)\n",
    "    for j, types in enumerate(type_annotations):\n",
    "        for t in types:\n",
    "            labels[j, config.type_vocab[t]] = 1\n",
    "    input_ent_cell_length = [len(x) if len(x)!=0 else 1 for x in input_ent_text]\n",
    "    max_cell_length = max(input_ent_cell_length)\n",
    "    input_ent_text_padded = np.zeros([len(input_ent_text), max_cell_length], dtype=int)\n",
    "    for i,x in enumerate(input_ent_text):\n",
    "        input_ent_text_padded[i, :len(x)] = x\n",
    "\n",
    "    return [table_id,np.array(input_tok),np.array(input_tok_type),np.array(input_tok_pos),(np.array(input_tok_mask[0]),np.array(input_tok_mask[1])),len(input_tok), \\\n",
    "                np.array(input_ent),input_ent_text_padded,input_ent_cell_length,np.array(input_ent_type),(np.array(input_ent_mask[0]),np.array(input_ent_mask[1])),len(input_ent), \\\n",
    "                column_header_mask,column_entity_mask,labels,len(labels)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_CT(testset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load type vocab from type_vocab.txt\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "test_dataset = WikiCTDataset(data_dir, entity_vocab, type_vocab, max_input_tok=500, src=\"test\", max_length = [50, 10, 10], force_new=False, tokenizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2type = {idx:t for t, idx in type_vocab.items()}\n",
    "t2d_invalid = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(output, relevance_labels):\n",
    "    with torch.no_grad():\n",
    "        sorted_output = torch.argsort(output, dim=-1, descending=True)\n",
    "        sorted_labels = torch.gather(relevance_labels, -1, sorted_output).float()\n",
    "        cum_correct = torch.cumsum(sorted_labels, dim=-1)\n",
    "        cum_precision = cum_correct / torch.arange(start=1,end=cum_correct.shape[-1]+1, device=cum_correct.device)[None, :]\n",
    "        cum_precision = cum_precision * sorted_labels\n",
    "        total_valid = torch.sum(sorted_labels, dim=-1)\n",
    "        total_valid[total_valid==0] = 1\n",
    "        average_precision = torch.sum(cum_precision, dim=-1)/total_valid\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## Evaluation TURL on TURL-Copus\n",
    "################################\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "from data_loader.CT_Wiki_data_loaders_STEER import WikiCTDataset, CTLoader\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "\n",
    "labeled_data_size = \"14.30\"\n",
    "unlabeled_data_size = \"65.70\"\n",
    "test_data_size = \"20.00\"\n",
    "random_state = 2\n",
    "\n",
    "data = WikiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=labeled_data_size, unlabeled_data_size=unlabeled_data_size, test_data_size=test_data_size,\n",
    "                    random_state=random_state, data_split_set=\"test\", max_input_tok=500, src=\"train_dev_test\", max_length=[50, 10, 10], force_new=False, tokenizer=None)\n",
    "\n",
    "for labeled_data_size, unlabeled_data_size in [(\"14.30\",\"65.70\"),(\"14.56\",\"65.44\"),(\"14.74\",\"65.26\"),(\"14.86\",\"65.14\"),(\"14.89\",\"65.11\")]:\n",
    "    train_sampler = SequentialSampler(data)\n",
    "    train_dataloader = CTLoader(data, sampler=train_sampler, batch_size=1, is_train=True)\n",
    "\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "    mode = 1\n",
    "    print(mode)\n",
    "    config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "    config = config_class.from_pretrained(config_name)\n",
    "    config.class_num = len(type_vocab)\n",
    "    config.mode = mode\n",
    "    model = model_class(config, is_simple=True)\n",
    "    #checkpoint = checkpoints[mode]\n",
    "    model_path = f\"output/CT/v2/{mode}/model_STEER_{labeled_data_size}_{unlabeled_data_size}_{test_data_size}_{random_state}_sameTraindata\"\n",
    "    checkpoint = torch.load(f\"{model_path}/pytorch_model.bin\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predicted_labels_all = []\n",
    "    true_labels_all = []\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # if step > 5:\n",
    "        #     break\n",
    "        table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "                input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "                column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        column_entity_mask = column_entity_mask.to(device)\n",
    "        column_header_mask = column_header_mask.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if mode == 1:\n",
    "            input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, column_entity_mask, column_header_mask, labels_mask, labels)\n",
    "            loss = outputs[0]\n",
    "            prediction_scores = outputs[1]\n",
    "            prediction_labels = (prediction_scores.view(-1, config.class_num)==prediction_scores.view(-1, config.class_num).max(-1)[0][:,None]).tolist()\n",
    "            true_labels = labels.view(-1, config.class_num).tolist()\n",
    "            predicted_labels_all.extend([pred.index(True) for pred in prediction_labels])\n",
    "            true_labels_all.extend([true_label.index(1.0) for true_label in true_labels])\n",
    "\n",
    "    class_report = classification_report(true_labels_all, predicted_labels_all, output_dict=True)\n",
    "    with open(f\"{model_path}/classification_report.json\", \"w\") as outfile:\n",
    "        json.dump(class_report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_size = 1\n",
    "unlabeled_data_size = \"absolute\"\n",
    "test_data_size = 20.0\n",
    "random_state = 2\n",
    "\n",
    "#data = WikiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=labeled_data_size, unlabeled_data_size=\"absolute\", test_data_size=test_data_size,\n",
    "#                     random_state=random_state, data_split_set=\"test\", max_input_tok=500, src=\"train_dev_test\", max_length=[50, 10, 10], force_new=False, tokenizer=None)\n",
    "\n",
    "train_sampler = SequentialSampler(data)\n",
    "train_dataloader = CTLoader(data, sampler=train_sampler, batch_size=1, is_train=True)\n",
    "\n",
    "\n",
    "\n",
    "epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "mode = 0\n",
    "print(mode)\n",
    "config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.class_num = len(type_vocab)\n",
    "config.mode = mode\n",
    "model = model_class(config, is_simple=True)\n",
    "#checkpoint = checkpoints[mode]\n",
    "model_path = f\"output/CT/v2/{mode}/model_STEER_{labeled_data_size}_absolute_{test_data_size}_{random_state}_STEER\"\n",
    "checkpoint = torch.load(f\"{model_path}/pytorch_model.bin\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predicted_labels_all = []\n",
    "true_labels_all = []\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    # if step > 5:\n",
    "    #     break\n",
    "    table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "            input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "            column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    input_ent_text = input_ent_text.to(device)\n",
    "    input_ent_text_length = input_ent_text_length.to(device)\n",
    "    input_ent = input_ent.to(device)\n",
    "    input_ent_type = input_ent_type.to(device)\n",
    "    input_ent_mask = input_ent_mask.to(device)\n",
    "    column_entity_mask = column_entity_mask.to(device)\n",
    "    column_header_mask = column_header_mask.to(device)\n",
    "    labels_mask = labels_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    if mode == 1:\n",
    "        input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, column_entity_mask, column_header_mask, labels_mask, labels)\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        prediction_labels = (prediction_scores.view(-1, config.class_num)==prediction_scores.view(-1, config.class_num).max(-1)[0][:,None]).tolist()\n",
    "        true_labels = labels.view(-1, config.class_num).tolist()\n",
    "        predicted_labels_all.extend([pred.index(True) for pred in prediction_labels])\n",
    "        true_labels_all.extend([true_label.index(1.0) for true_label in true_labels])\n",
    "\n",
    "class_report = classification_report(true_labels_all, predicted_labels_all, output_dict=True)\n",
    "with open(f\"{model_path}/classification_report.json\", \"w\") as outfile:\n",
    "    json.dump(class_report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "checkpoints = [\n",
    "     \"output/CT/v2/0/model_STEER_1_absolute_20.0_2/pytorch_model.bin\",\n",
    "     \"output/CT/v2/0/model_STEER_2_absolute_20.0_2/pytorch_model.bin\"\n",
    "#    \"output/CT/v2/0/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/checkpoint-50000/pytorch_model.bin\",\n",
    "#    \"output/CT/v2/1/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "#    \"output/CT/v2/2/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "#    \"output/CT/v2/3/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "#    \"output/CT/v2/4/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "#    \"output/CT/v2/5/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\"\n",
    "]\n",
    "# checkpoints = [\n",
    "#     #\"output/CT/v2/0/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "#     \"output/provided/column_type/0/pytorch_model.bin\",\n",
    "#     \"output/provided/column_type/1/pytorch_model.bin\"\n",
    "# ]\n",
    "# for mode in range(6):\n",
    "for mode in [0,0]:\n",
    "    print(mode)\n",
    "    config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "    config = config_class.from_pretrained(config_name)\n",
    "    config.class_num = len(type_vocab)\n",
    "    config.mode = mode\n",
    "    model = model_class(config, is_simple=True)\n",
    "    checkpoint = checkpoints[mode]\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    eval_batch_size = 20\n",
    "    eval_sampler = SequentialSampler(data)\n",
    "    eval_dataloader = CTLoader(data, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "    eval_loss = 0.0\n",
    "    eval_map = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    eval_targets = []\n",
    "    eval_prediction_scores = []\n",
    "    eval_pred = []\n",
    "    eval_mask = []\n",
    "    per_table_result[mode] = {}\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "            input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "            column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        column_entity_mask = column_entity_mask.to(device)\n",
    "        column_header_mask = column_header_mask.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if mode == 1:\n",
    "            input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "        elif mode == 2:\n",
    "            input_tok_mask = input_tok_mask[:,:,:input_tok_mask.shape[1]]\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "            input_ent = None\n",
    "            input_ent_type = None\n",
    "            input_ent_mask = None\n",
    "        elif mode == 3:\n",
    "            input_ent = None\n",
    "        elif mode == 4:\n",
    "            input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent = None\n",
    "        elif mode == 5:\n",
    "            input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, column_entity_mask, column_header_mask, labels_mask, labels)\n",
    "            loss = outputs[0]\n",
    "            prediction_scores = outputs[1]\n",
    "            for l_i in t2d_invalid:\n",
    "                prediction_scores[:,:,l_i] = -1000\n",
    "            for idx, table_id in enumerate(table_ids):\n",
    "                valid = labels_mask[idx].nonzero().max().item()+1\n",
    "                if table_id not in per_table_result[mode]:\n",
    "                    per_table_result[mode][table_id] = [[],labels_mask[idx,:valid],labels[idx,:valid]]\n",
    "                per_table_result[mode][table_id][0].append(prediction_scores[idx,:valid])\n",
    "            ap = metric.average_precision(prediction_scores.view(-1, config.class_num), labels.view((-1, config.class_num)))\n",
    "            map = (ap*labels_mask.view(-1)).sum()/labels_mask.sum()\n",
    "            eval_loss += loss.mean().item()\n",
    "            eval_map += map.item()\n",
    "            eval_targets.extend(labels.view(-1, config.class_num).tolist())\n",
    "            eval_prediction_scores.extend(prediction_scores.view(-1, config.class_num).tolist())\n",
    "            eval_pred.extend((torch.sigmoid(prediction_scores.view(-1, config.class_num))>0.5).tolist())\n",
    "            eval_mask.extend(labels_mask.view(-1).tolist())\n",
    "        nb_eval_steps += 1\n",
    "    print(eval_map/nb_eval_steps)\n",
    "    eval_targets = np.array(eval_targets)\n",
    "    eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "    eval_mask = np.array(eval_mask)\n",
    "    eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "    eval_pred = np.array(eval_pred)\n",
    "    eval_tp = eval_mask[:,np.newaxis]*eval_pred*eval_targets\n",
    "    eval_precision = np.sum(eval_tp,axis=0)/np.sum(eval_mask[:,np.newaxis]*eval_pred,axis=0)\n",
    "    eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "    eval_recall = np.sum(eval_tp,axis=0)/np.sum(eval_mask[:,np.newaxis]*eval_targets,axis=0)\n",
    "    eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "    eval_f1 = 2*eval_precision*eval_recall/(eval_precision+eval_recall)\n",
    "    eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "    per_type_instance_num = np.sum(eval_mask[:,np.newaxis]*eval_targets,axis=0)\n",
    "    per_type_instance_num[per_type_instance_num==0] = 1\n",
    "    per_type_correct_instance_num = np.sum(eval_mask[:,np.newaxis]*(eval_prediction_ranks<eval_targets.sum(axis=1)[:,np.newaxis])*eval_targets,axis=0)\n",
    "    per_type_accuracy[mode] = per_type_correct_instance_num/per_type_instance_num\n",
    "    per_type_precision[mode] = eval_precision\n",
    "    per_type_recall[mode] = eval_recall\n",
    "    per_type_f1[mode] = eval_f1\n",
    "    precision[mode] = np.sum(eval_tp)/np.sum(eval_mask[:,np.newaxis]*eval_pred)\n",
    "    recall[mode] = np.sum(eval_tp)/np.sum(eval_mask[:,np.newaxis]*eval_targets)\n",
    "    f1[mode] = 2*precision[mode]*recall[mode]/(precision[mode]+recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr = 0\n",
    "total_valid = 0\n",
    "errors = []\n",
    "for table_id, result in per_table_result[0].items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    current_corr = 0\n",
    "    for col_idx, pred in enumerate(prediction_scores.argmax(-1).tolist()):\n",
    "        current_corr += label[col_idx, pred].item()\n",
    "    total_valid += label_mask.sum().item()\n",
    "    total_corr += current_corr\n",
    "    if current_corr!=label_mask.sum().item():\n",
    "        errors.append(table_id)\n",
    "print(total_corr/total_valid, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t,i in sorted(type_vocab.items(),key=lambda z:-per_type_instance_num[z[1]]):\n",
    "    #print('%s %.4f %.4f %.4f %.4f %.4f  %.4f %.4f'%(t, per_type_instance_num[i], per_type_f1[0][i], per_type_f1[4][i], per_type_f1[1][i], per_type_f1[3][i], per_type_f1[2][i], per_type_f1[5][i]))\n",
    "    print('%s %.4f %.4f %.4f'%(t, per_type_instance_num[i], per_type_f1[0][i], per_type_f1[1][i] ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type mapping is used to map the types used in some other datasets to our types, so we can directly evaluate without retraining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    'Election': ['government.election'],\n",
    "    'Film': ['film.film'],\n",
    "    'mountain': ['geography.mountain'],\n",
    "    'Building': ['architecture.building'],\n",
    "    'RadioStation': ['broadcast.radio_station'],\n",
    "    'TelevisionShow': ['tv.tv_program'],\n",
    "    'Country': ['location.country'],\n",
    "    'Airport': ['aviation.airport'],\n",
    "    'AdministrativeRegion': ['location.region'],\n",
    "    'University': ['education.university'],\n",
    "    'Newspaper': ['book.newspaper'],\n",
    "    'FictionalCharacter': ['fictional_universe.fictional_character'],\n",
    "    'Currency': ['finance.currency'],\n",
    "    'Novel': ['book.book'],\n",
    "    'Wrestler': ['sports.pro_athlete'],\n",
    "    'swimmer': ['sports.pro_athlete'],\n",
    "    'GolfPlayer': ['sports.golfer', 'sports.pro_athlete'],\n",
    "    'Book': ['book.book'],\n",
    "    'Political Party': ['government.political_party'],\n",
    "    'Person': ['people.person'],\n",
    "    'VideoGame': ['cvg.computer_videogame'],\n",
    "    'Animal': ['biology.animal'],\n",
    "    'PoliticalParty': ['government.political_party'],\n",
    "    'BaseballPlayer': ['sports.pro_athlete'],\n",
    "    'Monarch': ['royalty.monarch'],\n",
    "    'Mountain': ['geography.mountain'],\n",
    "    'City': ['location.citytown'],\n",
    "    'Company': ['business.consumer_company'],\n",
    "    'cricketer': ['sports.pro_athlete'],\n",
    "    'Airline': ['aviation.airline']\n",
    "}\n",
    "t2d_types = set([y for _,x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t,i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    'City': ['location.citytown'],\n",
    "    'VideoGame': ['cvg.computer_videogame'],\n",
    "    'Mountain': ['geography.mountain'],\n",
    "    'Museum': [],\n",
    "    'Writer': ['film.writer', 'tv.tv_writer', 'music.writer', 'book.author'],\n",
    "    'Lake': [], \n",
    "    'AdministrativeRegion': ['location.administrative_division'],\n",
    "    'Book': ['book.book'],\n",
    "    'Saint': [],\n",
    "    'Monarch': ['royalty.monarch'],\n",
    "    'Bird': [],\n",
    "    'Plant': [],\n",
    "    'Mayor': [],\n",
    "    'Currency': ['finance.currency'],\n",
    "    'MovieDirector': ['film.director'],\n",
    "    'Company': ['film.production_company', 'automotive.company', 'business.consumer_company', 'business.defunct_company', ],\n",
    "    'Genre': ['cvg.cvg_genre', 'film.film_genre', 'broadcast.genre', 'media_common.media_genre', 'tv.tv_genre', 'music.genre'],\n",
    "    'GovernmentType': ['government.governmental_body'],\n",
    "    'Hospital': [],\n",
    "    'Building': ['architecture.building'],\n",
    "    'PoliticalParty': ['government.political_party'],\n",
    "    'Language': ['language.human_language'],\n",
    "    'Country': ['location.country'],\n",
    "    'University': ['education.university'],\n",
    "    'SportsTeam': ['sports.sports_team'],\n",
    "    'RadioStation': ['broadcast.radio_station'],\n",
    "    'Airport': ['aviation.airport'],\n",
    "    'Airline': ['aviation.airline'],\n",
    "    'Wrestler': [],\n",
    "    'Newspaper': ['book.newspaper'],\n",
    "    'Mammal': [],\n",
    "    'MountainRange': [],\n",
    "    'BaseballPlayer': ['baseball.baseball_player'],\n",
    "    'AcademicJournal': [],\n",
    "    'Scientist': [],\n",
    "    'Continent': [],\n",
    "    'Film': ['film.film']\n",
    "}\n",
    "\n",
    "t2d_types = set([y for _,x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t,i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    'Film': ['film.film'],\n",
    "    'Lake': [],\n",
    "    'Language': ['language.human_language'],\n",
    "    'Country': ['location.country'],\n",
    "    'Company': ['film.production_company', 'automotive.company', 'business.consumer_company', 'business.defunct_company'],\n",
    "    'Person': ['people.person'],\n",
    "    'VideoGame': ['cvg.computer_videogame'],\n",
    "    'City': ['location.citytown'],\n",
    "    'Currency': ['finance.currency'],\n",
    "    'Bird': [],\n",
    "    'Mountain': ['geography.mountain'],\n",
    "    'Scientist': [],\n",
    "    'Plant': [],\n",
    "    'TelevisionShow': ['tv.tv_program'],\n",
    "    'Animal': [],\n",
    "    'AdministrativeRegion': ['location.administrative_division'],\n",
    "    'Genre': ['cvg.cvg_genre', 'film.film_genre', 'broadcast.genre', 'media_common.media_genre', 'tv.tv_genre', 'music.genre'],\n",
    "    'Newspaper': ['book.newspaper'],\n",
    "    'Airport': ['aviation.airport'],\n",
    "    'AcademicJournal': [],\n",
    "    'PopulatedPlace': [],\n",
    "    'Wrestler': [],\n",
    "    'PoliticalParty': ['government.political_party'],\n",
    "    'Cricketer': ['cricket.cricket_player'],\n",
    "    'Eukaryote': [],\n",
    "    'Saint': [],\n",
    "    'Writer': ['film.writer', 'tv.tv_writer', 'music.writer', 'book.author'],\n",
    "    'Museum': [],\n",
    "    'BaseballPlayer': ['baseball.baseball_player'],\n",
    "    'EducationalInstitution': ['education.educational_institution'],\n",
    "    'GovernmentType': ['government.governmental_body'],\n",
    "    'SportsTeam': ['sports.sports_team'],   \n",
    "}\n",
    "\n",
    "reverse_type_mapping = {t2:t1 for t1,t2s in t2d_type_mapping.items() for t2 in t2s}\n",
    "\n",
    "t2d_types = set([y for _,x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t,i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "pred = 0\n",
    "tp = 0\n",
    "for table_id, result in per_table_result[4].items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    current_corr = 0\n",
    "    for col_idx in range(label.shape[0]):\n",
    "        if label_mask[col_idx]!=0:\n",
    "            gt_t = set([reverse_type_mapping[id2type[t]] for t in label[col_idx].nonzero()[0].tolist()])\n",
    "            if (prediction_scores[col_idx]>0).nonzero().shape[0]>0:\n",
    "                pred_t = set([reverse_type_mapping[id2type[t]] for t in (prediction_scores[col_idx]>0).nonzero()[0].tolist()])\n",
    "            else:\n",
    "                pred_t = set()\n",
    "            p += len(gt_t)\n",
    "            pred += len(pred_t)\n",
    "            tp += len(gt_t&pred_t)\n",
    "precision = tp/pred\n",
    "recall = tp/p\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(f1,precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[1].nonzero()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 if label_mask[1]==0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_table_result['64499281_8_7181683886563136802'][0][1].argsort(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT - Semtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Semtab'\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "test_dataset = WikiCTDataset(data_dir, entity_vocab, type_vocab, max_input_tok=500, src=\"wiki_test30\", max_length = [50, 10, 10], force_new=False, tokenizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(type_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2type = {y:x for x,y in type_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(output, relevance_labels):\n",
    "    with torch.no_grad():\n",
    "        sorted_output = torch.argsort(output, dim=-1, descending=True)\n",
    "        sorted_labels = torch.gather(relevance_labels, -1, sorted_output).float()\n",
    "        cum_correct = torch.cumsum(sorted_labels, dim=-1)\n",
    "        cum_precision = cum_correct / torch.arange(start=1,end=cum_correct.shape[-1]+1, device=cum_correct.device)[None, :]\n",
    "        cum_precision = cum_precision * sorted_labels\n",
    "        total_valid = torch.sum(sorted_labels, dim=-1)\n",
    "        total_valid[total_valid==0] = 1\n",
    "        average_precision = torch.sum(cum_precision, dim=-1)/total_valid\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_type_accuracy = {}\n",
    "per_type_precision = {}\n",
    "per_type_recall = {}\n",
    "per_type_f1 = {}\n",
    "map = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "checkpoint = \"output/CT/Semtab/wiki_train70/4/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\"\n",
    "mode = 4\n",
    "print(mode)\n",
    "config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.class_num = len(type_vocab)\n",
    "config.mode = mode\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_batch_size = 20\n",
    "eval_sampler = SequentialSampler(test_dataset)\n",
    "eval_dataloader = CTLoader(test_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "eval_loss = 0.0\n",
    "eval_map = 0.0\n",
    "nb_eval_steps = 0\n",
    "eval_targets = []\n",
    "eval_prediction_scores = []\n",
    "eval_pred = []\n",
    "eval_mask = []\n",
    "per_table_result = {}\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "        input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "        column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    input_ent_text = input_ent_text.to(device)\n",
    "    input_ent_text_length = input_ent_text_length.to(device)\n",
    "    input_ent = input_ent.to(device)\n",
    "    input_ent_type = input_ent_type.to(device)\n",
    "    input_ent_mask = input_ent_mask.to(device)\n",
    "    column_entity_mask = column_entity_mask.to(device)\n",
    "    column_header_mask = column_header_mask.to(device)\n",
    "    labels_mask = labels_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    if mode == 1:\n",
    "        input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "    elif mode == 2:\n",
    "        input_tok_mask = input_tok_mask[:,:,:input_tok_mask.shape[1]]\n",
    "        input_ent_text = None\n",
    "        input_ent_text_length = None\n",
    "        input_ent = None\n",
    "        input_ent_type = None\n",
    "        input_ent_mask = None\n",
    "    elif mode == 3:\n",
    "        input_ent = None\n",
    "    elif mode == 4:\n",
    "        input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "        input_ent = None\n",
    "    elif mode == 5:\n",
    "        input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "        input_ent_text = None\n",
    "        input_ent_text_length = None\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, column_entity_mask, column_header_mask, labels_mask, labels)\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        for idx, table_id in enumerate(table_ids):\n",
    "            valid = labels_mask[idx].nonzero().max().item()+1\n",
    "            if table_id not in per_table_result:\n",
    "                per_table_result[table_id] = [[],labels_mask[idx,:valid],labels[idx,:valid]]\n",
    "            per_table_result[table_id][0].append(prediction_scores[idx,:valid])\n",
    "        \n",
    "        eval_loss += loss.mean().item()\n",
    "        eval_targets.extend(labels.view(-1, config.class_num).tolist())\n",
    "        eval_prediction_scores.extend(prediction_scores.view(-1, config.class_num).tolist())\n",
    "        eval_pred.extend((prediction_scores.view(-1, config.class_num)==prediction_scores.view(-1, config.class_num).max(-1)[0][:,None]).tolist())\n",
    "        eval_mask.extend(labels_mask.view(-1).tolist())\n",
    "    nb_eval_steps += 1\n",
    "eval_targets = np.array(eval_targets)\n",
    "eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "eval_mask = np.array(eval_mask)\n",
    "eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "eval_pred = np.array(eval_pred)\n",
    "eval_tp = eval_mask[:,np.newaxis]*eval_pred*eval_targets\n",
    "eval_precision = np.sum(eval_tp,axis=0)/np.sum(eval_mask[:,np.newaxis]*eval_pred,axis=0)\n",
    "eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "eval_recall = np.sum(eval_tp,axis=0)/np.sum(eval_mask[:,np.newaxis]*eval_targets,axis=0)\n",
    "eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "eval_f1 = 2*eval_precision*eval_recall/(eval_precision+eval_recall)\n",
    "eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "per_type_instance_num = np.sum(eval_mask[:,np.newaxis]*eval_targets,axis=0)\n",
    "per_type_instance_num[per_type_instance_num==0] = 1\n",
    "per_type_correct_instance_num = np.sum(eval_mask[:,np.newaxis]*(eval_prediction_ranks<eval_targets.sum(axis=1)[:,np.newaxis])*eval_targets,axis=0)\n",
    "per_type_accuracy[mode] = per_type_correct_instance_num/per_type_instance_num\n",
    "per_type_precision[mode] = eval_precision\n",
    "per_type_recall[mode] = eval_recall\n",
    "per_type_f1[mode] = eval_f1\n",
    "precision[mode] = np.sum(eval_tp)/np.sum(eval_mask[:,np.newaxis]*eval_pred)\n",
    "recall[mode] = np.sum(eval_tp)/np.sum(eval_mask[:,np.newaxis]*eval_targets)\n",
    "f1[mode] = 2*precision[mode]*recall[mode]/(precision[mode]+recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_types = ['City', 'VideoGame', 'Mountain', 'Writer', 'Lake', 'AdministrativeRegion', 'Book', 'Saint', 'Monarch', 'Bird', 'Plant', 'Currency', 'Company', 'Genre', 'Building', 'PoliticalParty', 'Language', 'Country', 'University', 'SportsTeam', 'RadioStation', 'Airport', 'Wrestler', 'Newspaper', 'Mammal', 'Mayor', 'AcademicJournal', 'Scientist', 'Continent', 'Film', 'BaseballPlayer']\n",
    "non_wiki_types = [x for x in type_vocab if x not in wiki_types]\n",
    "wiki_types = set([type_vocab[x] for x in wiki_types])\n",
    "wiki_type_mask = torch.full((len(type_vocab),),-10000.0).to(device)\n",
    "for i in wiki_types:\n",
    "    wiki_type_mask[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t,i in type_vocab.items():\n",
    "    print(t, per_type_f1[4][i],per_type_instance_num[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_wiki_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr = 0\n",
    "total_valid = 0\n",
    "errors = []\n",
    "for table_id, result in per_table_result.items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    prediction_scores[:,15] = torch.where(prediction_scores[:,15]>prediction_scores[:,27],prediction_scores[:,15],prediction_scores[:,27])\n",
    "    prediction_scores += wiki_type_mask[None, :]\n",
    "    pred_acc = ((prediction_scores==prediction_scores.max(-1)[0][:,None])*label).sum(-1)\n",
    "    total_valid += label_mask.sum().item()\n",
    "    total_corr += pred_acc.sum().item()\n",
    "    if pred_acc.sum().item()!=label_mask.sum().item():\n",
    "        errors.append(table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for inspect_id in errors:\n",
    "    print(inspect_id)\n",
    "    prediction_scores, label_mask, label = per_table_result[inspect_id]\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    for col_id in range(label.shape[0]):\n",
    "        if label[col_id].sum().item()==0:\n",
    "            continue\n",
    "        display(id2type[label[col_id].nonzero().item()])\n",
    "        display([[id2type[l],prediction_scores[col_id,l].item()] for l in prediction_scores[col_id].argsort().tolist()[::-1][:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_table_result['Baseball_Hall_of_Fame_balloting,_2015#0'][0][0].argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PublicBI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import os\n",
    "os.environ[\"WORKING_DIR\"] = \"/home/sanonymous/semantic_data_lake\"\n",
    "os.environ[\"TYPENAME\"] = \"type78\"\n",
    "os.environ[\"PUBLIC_BI_BENCHMARK\"] = \"/ext/daten-wi/sanonymous/public_bi_benchmark/benchmark\"\n",
    "corpus = \"public_bi\"\n",
    "data_dir= \"data/public_bi/\"\n",
    "from model.transformers import BertTokenizer\n",
    "\n",
    "valid_header_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\",\n",
    "                         \"valid_headers\")\n",
    "\n",
    "labeled_unlabeled_test_split_path = join(os.environ[\"WORKING_DIR\"], \"data\",\n",
    "                                         \"extract\", \"out\",\n",
    "                                         \"labeled_unlabeled_test_split\")\n",
    "\n",
    "from sql_metadata import Parser\n",
    "\n",
    "def getPublicBIColumnNames(domain, table):\n",
    "    fd = open(join(os.environ[\"PUBLIC_BI_BENCHMARK\"], domain, \"tables\", f\"{table}.table.sql\"),\"r\")\n",
    "    sqlStmt = fd.read()\n",
    "    fd.close()\n",
    "    #print(sqlStmt)\n",
    "    #res = sql_metadata.get_query_tokens(sqlStmt)\n",
    "    columns = Parser(sqlStmt).columns\n",
    "    return columns\n",
    "\n",
    "def load_public_bi_table(domain, tablename, cols, number_of_rows, random_state, with_header=True):\n",
    "    df = pd.read_csv(os.path.join(os.environ.get(\"PUBLIC_BI_BENCHMARK\"),\n",
    "                                  domain, tablename + \".csv\"),\n",
    "                     sep=\"|\", on_bad_lines=\"skip\",\n",
    "                     header=None, usecols=cols)#.dropna().sample(n=number_of_rows, random_state=random_state)\n",
    "    if with_header:\n",
    "        # df_header = pd.read_csv(os.path.join(\n",
    "        #     os.environ.get(\"PUBLIC_BI_BENCHMARK\"), domain, \"samples\",\n",
    "        #     tablename + \".header.csv\"),\n",
    "        #                         sep=\"|\")\n",
    "        # df.columns = df_header.columns\n",
    "        columnNames = getPublicBIColumnNames(domain, tablename)\n",
    "        df.columns = [columnNames[i] for i in cols]\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "class PublicBiCTDataset(Dataset):\n",
    "\n",
    "    def _preprocess(self):\n",
    "        print(\"creating data...\")\n",
    "        data = []\n",
    "\n",
    "        for index, table in tqdm(enumerate(self.df[\"table_id\"].unique()), total=len(self.df[\"table_id\"].unique())):\n",
    "            if index > 0:\n",
    "                break\n",
    "            table_id = None \n",
    "            pgTitle = None \n",
    "            pgEnt = None \n",
    "            secTitle = None \n",
    "            caption = None\n",
    "            headers = None \n",
    "            entities = [] \n",
    "            type_annotations = None\n",
    "\n",
    "            cols = self.df[self.df[\"table_id\"] == table][\"column\"].tolist()\n",
    "            cols.sort()\n",
    "            df_table = load_public_bi_table(table.split(\"_\")[0], table, cols, 100, 2)\n",
    "\n",
    "            for i_col, col in enumerate(df_table.columns):\n",
    "                entities.append([[[row_i, i_col], [None,row_val]] for row_i, row_val in enumerate(df_table[col].tolist())])\n",
    "            \n",
    "            # assign semantic types to the column using valid_headers.json\n",
    "            type_annotations = [[self.valid_headers[table][f\"column_{col}\"][\"semanticType\"]] for col in cols]\n",
    "            data.append([table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations])\n",
    "\n",
    "        print(f\"{len(data)} tables loaded\")\n",
    "        pool = Pool(processes=4)\n",
    "        processed_cols = list(tqdm(pool.imap(partial(\n",
    "            process_single_CT, config=self), cols, chunksize=1000), total=len(cols)))\n",
    "        pool.close()\n",
    "        return data\n",
    "\n",
    "    def __init__(self, data_dir, entity_vocab, type_vocab, labeled_data_size, unlabeled_data_size, test_data_size, random_state, data_split_set=\"labeled\", add_STEER_train_data=False, max_column=10, max_input_tok=500, max_length=[50, 10, 10], force_new=False, tokenizer=None):\n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.force_new = force_new\n",
    "        self.max_input_tok = max_input_tok\n",
    "        self.max_title_length = max_length[0]\n",
    "        self.max_header_length = max_length[1]\n",
    "        self.max_cell_length = max_length[2]\n",
    "        self.max_column = max_column\n",
    "        self.entity_vocab = entity_vocab\n",
    "        self.entity_wikid2id = {\n",
    "            self.entity_vocab[x]['wiki_id']: x for x in self.entity_vocab}\n",
    "        self.type_vocab = type_vocab\n",
    "        self.type_num = len(self.type_vocab)\n",
    "        # STEER arguments\n",
    "        self.labeled_data_size = labeled_data_size\n",
    "        self.unlabeled_data_size = unlabeled_data_size\n",
    "        self.test_data_size = test_data_size\n",
    "        self.random_state = random_state\n",
    "        self.data_split_set = data_split_set\n",
    "        self.add_STEER_train_data = add_STEER_train_data\n",
    "\n",
    "        # load the valid headers with real sem. types\n",
    "        valid_header_file = f\"{corpus}_{os.environ['TYPENAME']}_valid.json\"\n",
    "        valid_headers = join(valid_header_path, valid_header_file)\n",
    "        with open(valid_headers, \"r\") as file:\n",
    "            self.valid_headers = json.load(file)\n",
    "\n",
    "        ## if additional training data by STEER, then overwrite the gold labels with the given labels from STEER Labeling Framework\n",
    "        def overwrite_labels_with_gen_train_labels(x):\n",
    "            self.valid_headers[x[\"table_id\"]][f\"column_{x['column']}\"][\"semanticType\"] = x[\"predicted_semantic_type\"]\n",
    "\n",
    "        # load labeled data from labeled, unlabeled, test split file\n",
    "        with open(join(labeled_unlabeled_test_split_path, f\"{corpus}_{self.labeled_data_size}_{self.unlabeled_data_size}_{self.test_data_size}_{self.random_state}.json\")) as f:\n",
    "            self.labeled_unlabeled_test_split_file = json.load(f)\n",
    "        if self.data_split_set == \"labeled\":\n",
    "            self.df = pd.DataFrame({\"table_id\": [entry.split(\"+\")[0] for entry in self.labeled_unlabeled_test_split_file[f\"labeled{self.labeled_data_size}\"]],\n",
    "                                    \"column\": [int(entry.split(\"+\")[1].split(\"_\")[1]) for entry in self.labeled_unlabeled_test_split_file[f\"labeled{self.labeled_data_size}\"]]})\n",
    "        elif self.data_split_set == \"test\":\n",
    "            self.df = pd.DataFrame({\"table_id\": [entry.split(\"+\")[0] for entry in self.labeled_unlabeled_test_split_file[f\"test{self.test_data_size}\"]],\n",
    "                                    \"column\": [int(entry.split(\"+\")[1].split(\"_\")[1]) for entry in self.labeled_unlabeled_test_split_file[f\"test{self.test_data_size}\"]]})\n",
    "\n",
    "        # load additional training data provided by STEER\n",
    "        if self.add_STEER_train_data == True:\n",
    "            self.df_gen_train_data = pd.read_csv(join(gen_train_path, f\"{corpus}_gen_training_data_all_combined_maj_{labeled_data_size}_absolute_{test_data_size}_{random_state}.csv\"), names=[\"table_id\", \"column\", \"dataset_id\", \"predicted_semantic_type\"], header=0)\n",
    "            self.df_gen_train_data[\"table_id\"] = self.df_gen_train_data[\"table_id\"].apply(lambda x: x.split(\".csv\")[0].split(\"_\")[1])\n",
    "            self.df_gen_train_data[\"column\"] = self.df_gen_train_data[\"column\"].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "            ## add additional train data to labeled columns\n",
    "            self.df = pd.concat([self.df, self.df_gen_train_data[[\"table_id\", \"column\"]]], ignore_index=True)\n",
    "\n",
    "            ## overwrite gold labeld with labels from STEER\n",
    "            self.df_gen_train_data.apply(lambda x: overwrite_labels_with_gen_train_labels(x), axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_public_bi_table(\"NYC\", \"NYC_2\", [2, 5, 7, 16, 18, 19, 27], 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entity vocab from entity_vocab.txt\n",
    "data_dir= \"data/public_bi/\"\n",
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "\n",
    "# train_data = PublicBiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=1,\n",
    "#                                unlabeled_data_size=\"absolute\", test_data_size=20.0, random_state=2, data_split_set=\"labeled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data/wikitables_v2/\", 'test.table_col_type.json'), 'r') as f:\n",
    "    testset = json.load(f)[4]\n",
    "table_id, pgTitle, pgEnt, secTitle, caption, headers, entities, type_annotations = testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "def load_data(config):\n",
    "    pass\n",
    "\n",
    "for index, table in enumerate(train_data.df[\"table_id\"].unique()):\n",
    "    if index > 0:\n",
    "        break\n",
    "    table_id = None \n",
    "    pgTitle = None \n",
    "    pgEnt = None \n",
    "    secTitle = None \n",
    "    caption = None\n",
    "    headers = None \n",
    "    entities = [] \n",
    "    type_annotations = None\n",
    "\n",
    "    print(table)\n",
    "    cols = train_data.df[train_data.df[\"table_id\"] == table][\"column\"].tolist()\n",
    "    cols.sort()\n",
    "    df_table = load_public_bi_table(table.split(\"_\")[0], table, cols, 100, 2)\n",
    "\n",
    "    for i_col, col in enumerate(df_table.columns):\n",
    "        entities.append([[[row_i, i_col], [None,row_val]] for row_i, row_val in enumerate(df_table[col].tolist())])\n",
    "    \n",
    "    # assign semantic types to the column using valid_headers.json\n",
    "    type_annotations = [train_data.valid_headers[table][f\"column_{col}\"][\"semanticType\"] for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.CT_PublicBI_data_loader import *\n",
    " \n",
    "train_data = PublicBiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=1,\n",
    "                                unlabeled_data_size=\"absolute\", test_data_size=20.0, random_state=2, data_split_set=\"labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Evaluation TURL on Public BI\n",
    "###############################\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "from data_loader.CT_PublicBI_data_loader import PublicBiCTDataset, CTLoader\n",
    "data_dir= \"data/public_bi/\"\n",
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "\n",
    "labeled_data_size = \"43.19\"\n",
    "unlabeled_data_size = \"36.80\"\n",
    "test_data_size = \"20.01\"\n",
    "random_state = 2\n",
    "\n",
    "data = PublicBiCTDataset(data_dir, entity_vocab, type_vocab, labeled_data_size=labeled_data_size, unlabeled_data_size=unlabeled_data_size, test_data_size=test_data_size,\n",
    "                    random_state=random_state, data_split_set=\"test\", max_input_tok=500, max_length=[50, 10, 10], force_new=False, tokenizer=None)\n",
    "\n",
    "\n",
    "for labeled_data_size, unlabeled_data_size in [(\"43.19\",\"36.80\"),(\"50.84\",\"29.14\"),(\"55.06\",\"24.93\"),(\"59.27\",\"20.72\"),(\"61.24\",\"18.75\")]:\n",
    "    train_sampler = SequentialSampler(data)\n",
    "    train_dataloader = CTLoader(data, sampler=train_sampler, batch_size=1, is_train=True)\n",
    "\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "    mode = 1\n",
    "    print(mode)\n",
    "    config_class, model_class, _ = MODEL_CLASSES['CT']\n",
    "    config = config_class.from_pretrained(config_name)\n",
    "    config.class_num = len(type_vocab)\n",
    "    config.mode = mode\n",
    "    model = model_class(config, is_simple=True)\n",
    "    #checkpoint = checkpoints[mode]\n",
    "\n",
    "    model_path = f\"output/CT/v2/{mode}/model_STEER_{labeled_data_size}_{unlabeled_data_size}_20.0_{random_state}_PublicBI_sameTraindata\"\n",
    "    checkpoint = torch.load(f\"{model_path}/pytorch_model.bin\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predicted_labels_all = []\n",
    "    true_labels_all = []\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # if step > 5:\n",
    "        #     break\n",
    "        table_ids, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "                input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, \\\n",
    "                column_entity_mask, column_header_mask, labels_mask, labels = batch\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        column_entity_mask = column_entity_mask.to(device)\n",
    "        column_header_mask = column_header_mask.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if mode == 1:\n",
    "            input_ent_mask = input_ent_mask[:,:,input_tok_mask.shape[1]:]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                input_ent_text, input_ent_text_length, input_ent, input_ent_type, input_ent_mask, column_entity_mask, column_header_mask, labels_mask, labels)\n",
    "            loss = outputs[0]\n",
    "            prediction_scores = outputs[1]\n",
    "            prediction_labels = (prediction_scores.view(-1, config.class_num)==prediction_scores.view(-1, config.class_num).max(-1)[0][:,None]).tolist()\n",
    "            true_labels = labels.view(-1, config.class_num).tolist()\n",
    "            predicted_labels_all.extend([pred.index(True) for pred in prediction_labels])\n",
    "            true_labels_all.extend([true_label.index(1.0) for true_label in true_labels])\n",
    "\n",
    "    class_report = classification_report(true_labels_all, predicted_labels_all, output_dict=True)\n",
    "    with open(f\"{model_path}/classification_report.json\", \"w\") as outfile:\n",
    "        json.dump(class_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine results of multiple runs with different random states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "add_comment = \"PublicBI\"\n",
    "for labeled_data_size in [1,2,3,4,5]:\n",
    "    path = join(\"output\", \"CT\", \"v2\", \"1\",\n",
    "        f\"model_STEER_{labeled_data_size}_absolute_20.0\"\n",
    "    )\n",
    "    scores = {\n",
    "        \"f1-scores_macro\": [],\n",
    "        \"precisions_macro\":[],\n",
    "        \"recalls_macro\":[],\n",
    "        \"supports_macro\": [],\n",
    "        \"f1-scores_weighted\": [],\n",
    "        \"precisions_weighted\": [],\n",
    "        \"recalls_weighted\": [],\n",
    "        \"supports_weighted\": []\n",
    "    }\n",
    "\n",
    "\n",
    "    for random_state in [1,2,3,4,5]:\n",
    "        with open(join(path+f\"_{random_state}_{add_comment}\", \"classification_report.json\" ), \"r\") as f:\n",
    "            current_class_report = json.load(f)\n",
    "        for metric in [\"macro\",\"weighted\"]:\n",
    "            scores[f\"f1-scores_{metric}\"].append(current_class_report[f\"{metric} avg\"][\"f1-score\"])\n",
    "            scores[f\"precisions_{metric}\"].append(current_class_report[f\"{metric} avg\"][\"precision\"])\n",
    "            scores[f\"recalls_{metric}\"].append(current_class_report[f\"{metric} avg\"][\"recall\"])\n",
    "            scores[f\"supports_{metric}\"].append(current_class_report[f\"{metric} avg\"][\"support\"])\n",
    "\n",
    "    df_scores = pd.DataFrame(\n",
    "        np.array([\n",
    "            scores[\"f1-scores_macro\"], scores[\"precisions_macro\"],\n",
    "            scores[\"recalls_macro\"], scores[\"supports_macro\"],\n",
    "            scores[\"f1-scores_weighted\"], scores[\"precisions_weighted\"],\n",
    "            scores[\"recalls_weighted\"], scores[\"supports_weighted\"]\n",
    "        ]), index=scores.keys())\n",
    "    df_scores[\"mean\"] = df_scores.mean(axis=1)\n",
    "    df_scores[\"std\"] = df_scores.std(axis=1)\n",
    "    df_scores[\"var\"] = df_scores.var(axis=1)\n",
    "\n",
    "    df_scores.to_csv(path+f\"{add_comment}_mean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
