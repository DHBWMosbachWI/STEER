{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment set-up\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "#load_dotenv(override=True)\n",
    "from os.path import join\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"/home/sanonymous/sato\")\n",
    "sys.path.append(\"/home/sanonymous/sato/model\")\n",
    "\n",
    "\n",
    "os.environ['BASEPATH'] = '/home/sanonymous/sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/anonymousmlc-ds12-v2/code/Users/svenanonymous/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type_public_bi'\n",
    "\n",
    "\n",
    "from model import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import get_valid_types\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(get_valid_types(os.environ[\"TYPENAME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AB'], dtype='<U14')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data in train_sherlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanonymous/sato/tmp/public_bi_num_type_public_bi_header_valid.pkl pickle file found, loading...\n",
      "public_bi_num_type_public_bi_header_valid Load complete. Time 0.0005321502685546875\n",
      "Total data preparation time: 0.17621922492980957\n",
      "Total time for splitting on column-level: 0.0019125938415527344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_list</th>\n",
       "      <th>field_names</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLB+MLB_1</th>\n",
       "      <td>[0, 4, 30, 35, 40]</td>\n",
       "      <td>[0, 1, 13, 56, 93]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   field_list         field_names\n",
       "table_id                                         \n",
       "MLB+MLB_1  [0, 4, 30, 35, 40]  [0, 1, 13, 56, 93]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model import models_sherlock\n",
    "from utils import name2dic\n",
    "\n",
    "topic = \"num-directstr_thr-0_tn-400\"\n",
    "\n",
    "if topic:\n",
    "    topic_dim = int(name2dic(topic)['tn'])\n",
    "else:\n",
    "    topic_dim = None\n",
    "\n",
    "corpus = \"public_bi_num\"\n",
    "percent = 5\n",
    "test_data_size = 20.0\n",
    "random_state = 2\n",
    "\n",
    "column_level_split_file_path = f\"/home/sanonymous/semantic_data_lake/data/extract/out/labeled_unlabeled_test_split/{corpus}_{percent}_absolute_{test_data_size}_{random_state}.json\"\n",
    "#embclus_gen_train_data_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\emb_clus\\\\knn_classifier\\\\out\\\\gen_training_data\\\\{corpus}_gen_training_data_1_{distance_threshold}_{percent}_absolute_{test_data_size}.csv\"\n",
    "embclus_gen_train_data_path = f\"/home/sanonymous/semantic_data_lake/labeling_functions/combined_LFs/gen_training_data/{corpus}_gen_training_data_all_combined_maj_{percent}_absolute_{test_data_size}_{random_state}.csv\"\n",
    "pretrained_shelock_path = f\"sherlock_retrain_embclus_all_combined_maj_labeled{percent}_unlabeledAbsolute_test{test_data_size}_{random_state}.pt\"\n",
    "                \n",
    "with open(column_level_split_file_path) as f:\n",
    "    labeled_unlabeled_test_split_file = json.load(f)\n",
    "\n",
    "#embclus_gen_training_data_path = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\emb_clus\\\\knn_classifier\\\\out\\\\gen_training_data\\\\gen_training_data_1_1e-06_10_70_20.csv\"\n",
    "df_embclus_training_data = pd.read_csv(embclus_gen_train_data_path)\n",
    "df_embclus_training_data[\"field_name\"] = label_enc.transform(df_embclus_training_data[\"predicted_semantic_type\"].tolist())\n",
    "#df_embclus_training_data\n",
    "\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(get_valid_types(os.environ[\"TYPENAME\"]))\n",
    "\n",
    "whole_corpus = datasets.TableFeatures(\"public_bi_num\",['char', 'rest', 'par', 'word'], topic_feature=topic, label_enc=label_enc, id_filter=None, max_col_count=100)\n",
    "\n",
    "train = copy.copy(whole_corpus)\n",
    "\n",
    "train = train.set_filter_for_column_level_split([\"MLB_1+column_4\", \"MLB_1+column_0\", \"MLB_1+column_30\", \"MLB_1+column_35\", \"MLB_1+column_40\"])\n",
    "\n",
    "train.df_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data in train_CRF_LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model import models_sherlock\n",
    "from utils import name2dic\n",
    "\n",
    "topic = \"num-directstr_thr-0_tn-400\"\n",
    "\n",
    "if topic:\n",
    "    topic_dim = int(name2dic(topic)['tn'])\n",
    "else:\n",
    "    topic_dim = None\n",
    "\n",
    "# load single column model\n",
    "pre_trained_sherlock_loc = join(\n",
    "    os.environ['BASEPATH'], 'model', 'pre_trained_sherlock', os.environ[\"TYPENAME\"])\n",
    "classifier = models_sherlock.build_sherlock(['char', 'rest', 'par', 'word'], num_classes=len(\n",
    "    get_valid_types(os.environ[\"TYPENAME\"])), topic_dim=topic_dim, dropout_ratio=0.35).to(device)\n",
    "\n",
    "classifier.load_state_dict(torch.load(\n",
    "            join(pre_trained_sherlock_loc, \"all_None.pt\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(get_valid_types(os.environ[\"TYPENAME\"]))\n",
    "\n",
    "whole_corpus = datasets.TableFeatures(\"public_bi_benchmark\",['char', 'rest', 'par', 'word'], topic_feature=topic, label_enc=label_enc, id_filter=None, max_col_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "train = copy.copy(whole_corpus).set_filter_for_column_level_split(labeled_unlabeled_test_split_file[\"labeled10\"]+labeled_unlabeled_test_split_file[\"unlabeled70\"])\n",
    "#train = copy.copy(whole_corpus).set_filter_for_column_level_split(['CityMaxCapita_1+column_1', \"CityMaxCapita_1+column_2\"])\n",
    "\n",
    "train_list.append(train)\n",
    "\n",
    "training_data = ConcatDataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]\n",
    "train.df_header\n",
    "df_header = copy.copy(train.df_header)\n",
    "#df_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.overwrite_col_labels_from_embclus(df_embclus_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_embclus_training_data.iterrows():\n",
    "    # if index > 2:\n",
    "    #     break\n",
    "    #print(index)\n",
    "    dataset_id = row[\"table\"]\n",
    "    locator = dataset_id.split(\"_\")[0]\n",
    "    table_id = locator+\"+\"+dataset_id\n",
    "    column_id = int(row[\"column\"].split(\"_\")[1])\n",
    "    \n",
    "    field_list = eval(df_header.loc[table_id][\"field_list\"])\n",
    "    field_names = eval(df_header.loc[table_id][\"field_names\"])\n",
    "\n",
    "    # selection in df_header\n",
    "    #print(df_header.loc[table_id])\n",
    "    #print(field_list.index(column_id))\n",
    "    searched_index = field_list.index(column_id)\n",
    "\n",
    "    # override field_names with the predicted semantic type\n",
    "    field_names[searched_index] = row[\"field_name\"]\n",
    "\n",
    "    #print(field_names)\n",
    "\n",
    "    df_header.loc[table_id][\"field_names\"] = str(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "train = copy.copy(whole_corpus).set_filter([\n",
    "    \"CommonGovernment+CommonGovernment_10\"\n",
    "])\n",
    "\n",
    "test = copy.copy(whole_corpus).set_filter([\n",
    "    #\"MLB+MLB_1\",\n",
    "    \"MLB+MLB_46\"\n",
    "])\n",
    "\n",
    "train_list.append(train)\n",
    "\n",
    "training_data = ConcatDataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_worker = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "training = datasets.generate_batches(training_data,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 drop_last=True,\n",
    "                                                 device=device,\n",
    "                                                 n_workers=n_worker)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import classification_report\n",
    "from torchcrf import CRF\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "training_iter = tqdm(training, desc=\"Training\")\n",
    "for table_batch, label_batch, mask_batch in training_iter:\n",
    "    for f_g in table_batch:\n",
    "        table_batch[f_g] = table_batch[f_g].view(batch_size * 100, -1)\n",
    "\n",
    "    pred_scores = classifier.predict(table_batch)\n",
    "    #print(pred_scores[0])\n",
    "    pred = torch.argmax(pred_scores, dim=1).cpu().numpy()\n",
    "    #print(pred)\n",
    "\n",
    "    labels = label_batch.view(-1).cpu().numpy()\n",
    "    masks = mask_batch.view(-1).cpu().numpy()\n",
    "    invert_masks = np.invert(masks == 1)\n",
    "    #print(labels)\n",
    "\n",
    "    y_pred.extend(ma.array(pred, mask=invert_masks).compressed())\n",
    "    y_true.extend(ma.array(labels, mask=invert_masks).compressed())\n",
    "\n",
    "    print(\"Predictions: \",y_pred)\n",
    "    print(\"True types: \",y_true)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "\n",
    "    emissions = classifier(table_batch).view(\n",
    "                    batch_size, 100, -1).to(device)\n",
    "    #print(\"Emissions: \",emissions)\n",
    "\n",
    "    model = CRF(len(get_valid_types(os.environ[\"TYPENAME\"])), batch_first=True).to(device)\n",
    "    # load pretrained model\n",
    "    model_loc = join(os.environ['BASEPATH'], 'model',\n",
    "                        'pre_trained_CRF', os.environ[\"TYPENAME\"])\n",
    "    loaded_params = torch.load(\n",
    "        join(model_loc, \"CRF+LDA_pathL.pt\"), map_location=device)\n",
    "    model.load_state_dict(loaded_params['CRF_model'])\n",
    "    model.eval()\n",
    "\n",
    "    pred = model.decode(emissions, mask_batch)\n",
    "    print(\"SATO Prediction:\")\n",
    "    print(pred)\n",
    "\n",
    "    print(classification_report(y_true, pred[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "labels = label_batch.view(-1).cpu().numpy()\n",
    "masks = mask_batch.view(-1).cpu().numpy()\n",
    "invert_masks = np.invert(masks == 1)\n",
    "\n",
    "ma.array(labels, mask=invert_masks).compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_batch[0][0] = 1\n",
    "mask_batch[0][1] = 1\n",
    "mask_batch[0][2] = 0\n",
    "mask_batch[0][3] = 0\n",
    "mask_batch[0][4] = 0\n",
    "print(mask_batch)\n",
    "print(mask_batch[0].all())\n",
    "print(mask_batch[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_loading_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look in table topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract.helpers.read_raw_data import get_filtered_dfs_by_corpus\n",
    "from extract.helpers.utils import valid_header_iter_gen\n",
    "\n",
    "\n",
    "corpus = \"public_bi_benchmark\"\n",
    "TYPENAME = \"type78\"\n",
    "header_name = \"{}_{}_header_valid.csv\".format(corpus, TYPENAME)\n",
    "header_iter = valid_header_iter_gen(header_name)\n",
    "\n",
    "print(header_iter)\n",
    "#raw_df_iter = get_filtered_dfs_by_corpus[\"publicbibenchmark\"](header_iter)\n",
    "\n",
    "for df_features in header_iter:\n",
    "    print(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "column_level_id_filter = [\n",
    "    \"MLB_1+column_37\",\n",
    "    \"MLB_1+column_45\", \n",
    "    \"MLB_46+column_52\",\n",
    "    \"MLB_46+column_58\"]\n",
    "\n",
    "df_dic = {}\n",
    "\n",
    "for training_set in column_level_id_filter:\n",
    "    dataset_id = training_set.split(\"+\")[0]\n",
    "    # this locator is only suitable for public_bi corpus\n",
    "    locator = dataset_id.split(\"_\")[0]\n",
    "    table_id = locator+\"+\"+dataset_id\n",
    "    column_id = int(training_set.split(\"+\")[1].split(\"_\")[1])\n",
    "    if table_id not in df_dic.keys():\n",
    "        df_dic[table_id] = [column_id]\n",
    "    elif table_id in df_dic.keys():\n",
    "        df_dic[table_id].append(column_id)\n",
    "\n",
    "# generate new df_header\n",
    "df_header_new = pd.DataFrame(\n",
    "    {\"table_id\": [], \"field_list\": [], \"field_names\": []})\n",
    "## manipulate data_dic\n",
    "new_data_dic = {}\n",
    "for f_g in old_data_dic.keys():\n",
    "    new_data_dic[f_g] = {}\n",
    "old_data_dic = whole_corpus.data_dic\n",
    "for table_id in df_dic.keys():\n",
    "    print(table_id)\n",
    "    # sort every field_list\n",
    "    df_dic[table_id] = sorted(df_dic[table_id])\n",
    "\n",
    "    old_field_list = np.array(eval(\n",
    "        whole_corpus.df_header.loc[table_id][\"field_list\"]))\n",
    "    print(old_field_list)\n",
    "\n",
    "    old_field_names = np.array(eval(\n",
    "        whole_corpus.df_header.loc[table_id][\"field_names\"]))\n",
    "\n",
    "    new_field_list = df_dic[table_id]\n",
    "    mask_fields = np.array([x in new_field_list for x in old_field_list])\n",
    "    new_field_names = old_field_names[mask_fields].tolist()\n",
    "\n",
    "    df_header_new = df_header_new.append(pd.DataFrame({\"table_id\": [table_id], \"field_list\": [\n",
    "                                            str(new_field_list)], \"field_names\": [str(new_field_names)]}), ignore_index=True)\n",
    "\n",
    "    \n",
    "    for f_g in old_data_dic.keys():\n",
    "        new_data_dic[f_g][table_id] = old_data_dic[f_g][table_id][np.argwhere(mask_fields == True).flatten()]\n",
    "    \n",
    "    \n",
    "df_header_new = df_header_new.set_index(\"table_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dic[\"rest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NN with different number of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_sherlock import FeatureEncoder, SherlockClassifier, build_sherlock\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sherlock_feature_groups = ['char', 'rest', 'par', 'word']\n",
    "\n",
    "\n",
    "classifier = build_sherlock(sherlock_feature_groups, num_classes=78, topic_dim=400, dropout_ratio=0.35).to(device)\n",
    "\n",
    "model_loc = join(os.environ['BASEPATH'],\n",
    "                         'model', 'pre_trained_sherlock', \"type78\")\n",
    "pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}_255types.pt\"\n",
    "\n",
    "classifier.load_state_dict(torch.load(join(model_loc,pretrained_shelock_path), map_location=device))\n",
    "print(classifier.linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.linear3.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.linear3.out_features = 255\n",
    "print(classifier.linear3)\n",
    "print(classifier.linear3.weight.data)\n",
    "print(len(classifier.linear3.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "classifier.linear3 = nn.Linear(500, 255)\n",
    "print(classifier.linear3.weight.data)\n",
    "print(len(classifier.linear3.weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain sherlock and check the las layer \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_anonymous_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/anonymousmlc-ds12-v2/code/Users/svenanonymous/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "percent = 50\n",
    "# if index != 0:\n",
    "#     continue\n",
    "comment = f\"labeled{percent}_unlabeled{100-20-percent}_test{20}_255types\"\n",
    "column_level_split_file_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_{percent}_{100-20-percent}_20.json\"\n",
    "pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}_255types.pt\"\n",
    "pretrained_CRF_LDA_path = f\"CRF+LDA_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "\n",
    "# retrain sherlock\n",
    "%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_retrain.txt  --comment {comment} --column_level_split_file_path {column_level_split_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}_255types.pt\"\n",
    "\n",
    "%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_eval.txt --model_list {pretrained_shelock_path} --comment \"sherlock_baseline_column_level_split_test20\" --column_level_split_file_path \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_10_70_20.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
