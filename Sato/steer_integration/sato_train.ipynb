{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# This Notebook is for training new Sato models with other datacorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment set-up\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"D:\\\\20120321_anonymous_AZUREML\\\\sato\")\n",
    "sys.path.append(\"D:\\\\20120321_anonymous_AZUREML\\\\sato\\\\model\")\n",
    "from os.path import join\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_anonymous_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/anonymousmlc-ds12-v2/code/Users/svenanonymous/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "# set requirements\n",
    "#from azureml.core import Workspace, Environment\n",
    "#ws = Workspace.from_config()\n",
    "#Environment(name='satoEnv')\n",
    "\n",
    "#satoEnv = Environment.from_pip_requirements(name=\"satoEnv\",file_path=\"../requirements.txt\")\n",
    "#satoEnv.register(workspace=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Column feature extraction\n",
    "%run ../extract/extract_features.py public_bi_benchmark -f sherlock -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Command Line Args:   --corpus_list public_bi_num\n",
      "Defaults:\n",
      "  --multi_col_only:  False\n",
      "\n",
      "----------\n",
      "['public_bi_num']\n",
      "Spliting public_bi_num\n",
      "dict_keys(['train5', 'test95', 'train10', 'test90', 'train15', 'test85', 'train20', 'test80', 'train25', 'test75', 'train30', 'test70', 'train35', 'test65', 'train40', 'test60', 'train45', 'test55', 'train50', 'test50', 'train55', 'test45', 'train60', 'test40', 'train65', 'test35', 'train70', 'test30', 'train75', 'test25', 'train80', 'test20', 'train85', 'test15', 'train90', 'test10', 'train95', 'test5', 'train100', 'test100'])\n",
      "Done, 136 training tables, 34 testing tables for 80/20 split\n",
      "Done, 170 training tables, 170 testing tables for 100/100 split\n"
     ]
    }
   ],
   "source": [
    "# split train test\n",
    "%run ../extract/split_train_test.py --corpus_list public_bi_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sato-Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation of Sato, first try in column-level split\n",
    "%run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_eval.txt --model_list \"model.pt\" \"CRF+LDA_pathL.pt\" \"CRF+LDA_pre.pt\" --train_percent \"train100\" --test_percent \"test100\" --comment \"sato_baseline_column_level_split_test20\" --column_level_split_file_path \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_10_70_20.json\"\n",
    "#%run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_eval.txt --model_list \"CRF_pre.pt\" \"CRF+LDA_retrain_train80_test20.pt\" --train_percent \"train100\" --test_percent \"test100\" --comment \"column-level-split\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Namespace(TYPENAME='type78', batch_size=1, column_level_split_file_path='D:\\\\\\\\semantic_data_lake\\\\\\\\semantic_data_lake\\\\\\\\data\\\\\\\\extract\\\\\\\\out\\\\\\\\labeled_unlabeled_test_split\\\\\\\\public_bi_10_70_20.json', comment='sherlock_baseline_column_level_split_test20', config_file='../model/params/publicbi/sherlock_eval.txt', corpus_list=['public_bi_benchmark'], cross_validation=None, decay=0.0001, dropout_rate=0.35, embclus_gen_train_data_path=None, epochs=100, learning_rate=0.0001, mode='eval', model_list=['all_None.pt'], multi_col_eval=False, multi_col_only=False, n_worker=4, patience=100, pretrain_model_path=None, sherlock_feature_groups=['char', 'rest', 'par', 'word'], test_percent='test100', topic='num-directstr_thr-0_tn-400', train_percent='train100')\n",
      "----------\n",
      "Command Line Args:   -c ../model/params/publicbi/sherlock_eval.txt --model_list all_None.pt --comment sherlock_baseline_column_level_split_test20 --column_level_split_file_path D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_10_70_20.json\n",
      "Environment Variables:\n",
      "  TYPENAME:          type78\n",
      "Config File (../model/params/publicbi/sherlock_eval.txt):\n",
      "  corpus_list:       ['public_bi_benchmark']\n",
      "  mode:              eval\n",
      "  topic:             num-directstr_thr-0_tn-400\n",
      "  batch_size:        1\n",
      "  train_percent:     train100\n",
      "  test_percent:      test100\n",
      "Defaults:\n",
      "  --n_worker:        4\n",
      "  --epochs:          100\n",
      "  --learning_rate:   0.0001\n",
      "  --decay:           0.0001\n",
      "  --dropout_rate:    0.35\n",
      "  --patience:        100\n",
      "  --sherlock_feature_groups:['char', 'rest', 'par', 'word']\n",
      "  --multi_col_only:  False\n",
      "  --multi_col_eval:  False\n",
      "\n",
      "----------\n",
      "PyTorch device=cpu\n",
      "\n",
      "logging_name sherlock_eval_sherlock_baseline_column_level_split_test20\n",
      "Creating Dataset object...\n",
      "data length:\n",
      "\n",
      "159 159\n",
      "D:\\20120321_anonymous_AZUREML\\sato\\tmp\\public_bi_benchmark_type78_header_valid.pkl pickle file found, loading...\n",
      "public_bi_benchmark_type78_header_valid Load complete. Time 0.01260519027709961\n",
      "Total data preparation time: 0.13097357749938965\n",
      "Time used to convert to SherlockDataset (column features) 0.02356410026550293\n",
      "Done (0 sec.)\n",
      "[Val] loss: 10.444389343261719\n",
      "[Val] acc: 0.3894736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anonymous\\.conda\\envs\\sato\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anonymous\\.conda\\envs\\sato\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.16438841236977883, 'recall': 0.14381940523244874, 'f1-score': 0.13662203085558633, 'support': 285} {'precision': 0.40490410809632776, 'recall': 0.3894736842105263, 'f1-score': 0.36754706024909933, 'support': 285}\n",
      "         model  macro avg  weighted avg\n",
      "0  all_None.pt   0.136622      0.367547\n",
      "Evaluation time 0 sec.\n"
     ]
    }
   ],
   "source": [
    "# validation of Sherlock, first try in column-level split\n",
    "%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_eval.txt --model_list \"all_None.pt\" --comment \"sherlock_baseline_column_level_split_test20\" --column_level_split_file_path \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_10_70_20.json\"\n",
    "#%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_eval.txt --model_list \"all_None.pt\" --comment \"sherlock_baseline_column_level_split_test20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sato + small set of labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.concatenate((np.arange(0,6,1),np.arange(6,11,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_anonymous_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/anonymousmlc-ds12-v2/code/Users/svenanonymous/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "#for index,percent in enumerate(np.arange(5,55,5)):\n",
    "for index, percent in enumerate(np.arange(1,11,1)):\n",
    "    # if index != 0:\n",
    "    #     continue\n",
    "    comment = f\"labeled{percent}_unlabeled{100-20-percent}_test{20}\"\n",
    "    column_level_split_file_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_{percent}_{100-20-percent}_20.json\"\n",
    "    pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "    pretrained_CRF_LDA_path = f\"CRF+LDA_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "    \n",
    "    # retrain sherlock\n",
    "    %run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_retrain.txt  --comment {comment} --column_level_split_file_path {column_level_split_file_path}\n",
    "    # retrain sato\n",
    "    %run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_retrain.txt --pre_trained_sherlock_path {pretrained_shelock_path} --comment {comment} --column_level_split_file_path {column_level_split_file_path}\n",
    "    # validate sato\n",
    "    %run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_eval.txt --model_list {pretrained_CRF_LDA_path} --comment {\"eval_\"+comment}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sato retrained with EmbClus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_anonymous_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/anonymousmlc-ds12-v2/code/Users/svenanonymous/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "distance_threshold = 0.1\n",
    "\n",
    "#for index,percent in enumerate(np.arange(5,55,5)):\n",
    "for index, percent in enumerate([1,2,3,4,6,7,8,9,10,15,20,25,30,35,40,45,50]):\n",
    "    #for distance_threshold in [1e-10,1e-9,1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e-0]:\n",
    "    for distance_threshold in [1e-6]:\n",
    "        # if index > 0:\n",
    "        #     break\n",
    "        comment = f\"embclus_{distance_threshold}_labeled{percent}_unlabeled{100-20-percent}_test{20}\"\n",
    "        column_level_split_file_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_{percent}_{100-20-percent}_20.json\"\n",
    "        embclus_gen_train_data_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\emb_clus\\\\knn_classifier\\\\out\\\\gen_training_data\\\\gen_training_data_1_{distance_threshold}_{percent}_{100-20-percent}_20.csv\"\n",
    "        pretrained_shelock_path = f\"sherlock_retrain_embclus_{distance_threshold}_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "        pretrained_CRF_LDA_path = f\"CRF+LDA_retrain_embclus_{distance_threshold}_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "        \n",
    "        # retrain sherlock\n",
    "        %run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_retrain.txt  --comment {comment} --column_level_split_file_path {column_level_split_file_path} --embclus_gen_train_data_path {embclus_gen_train_data_path}\n",
    "        # retrain sato\n",
    "        %run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_retrain.txt --pre_trained_sherlock_path {pretrained_shelock_path} --comment {comment} --column_level_split_file_path {column_level_split_file_path} --embclus_gen_train_data_path {embclus_gen_train_data_path}\n",
    "        # validate sato\n",
    "        %run ../model/train_CRF_LC.py -c ../model/params/publicbi/CRF+LDA_eval.txt --model_list {pretrained_CRF_LDA_path} --comment {\"eval_\"+comment}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header-Valid File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# find out what a ..header_valid.csv is by analyzing the .pkl files\n",
    "import pickle\n",
    "\n",
    "with open(\"../tmp/manyeyes_type78_header_valid.pkl\",\"rb\") as fin:\n",
    "    data_dic = pickle.load(fin)\n",
    "type(data_dic)\n",
    "data_dic.to_csv(\"manyeyes_type78_header_valid.csv\")\n",
    "\n",
    "#pickle.load(open(\"../tmp/webtables0-p1_type78_header_valid.pkl\",\"rb\"))\n",
    "#pickle.load(open(\"../tmp/manyeyes_sherlock_features.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.environ['RAW_DIR']"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
